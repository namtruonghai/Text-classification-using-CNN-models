{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN.LAPTOP-6P42KB5P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADMIN.LAPTOP-6P42KB5P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFBertModel\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import TFRobertaModel\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.layers import Dense, ReLU, Flatten, LSTM, Input, MaxPooling1D, Lambda, MaxPooling2D\n",
    "from tensorflow.keras.layers import Input, Embedding, BatchNormalization, Concatenate, Reshape\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, Conv2D, SpatialDropout1D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dữ liệu cần train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Twitter_3/data_train.csv',encoding='ISO-8859-1')\n",
    "df_test=pd.read_csv('Twitter_3/data_test.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tokenizers của BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "token_lens = []\n",
    "\n",
    "for txt in df['clean_text'].values:\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens.append(len(tokens))\n",
    "    \n",
    "max_len=np.max(token_lens)\n",
    "token_lens_test = []\n",
    "\n",
    "for txt in df_test['clean_text'].values:\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens_test.append(len(tokens))\n",
    "    \n",
    "max_len=np.max(token_lens_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROS over sampler giúp cho mô hình chạy ổn định hơn va chia tap train test de chay mo hinh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros=RandomOverSampler()\n",
    "train_x, train_y = ros.fit_resample(np.array(df['text_clean']).reshape(-1, 1), np.array(df['Sentiment']).reshape(-1, 1))\n",
    "train_os = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['clean_text', 'Sentiment'])\n",
    "X = train_os['clean_text'].values\n",
    "y = train_os['Sentiment'].values\n",
    "seed=42\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, stratify=y, random_state=seed)\n",
    "X_test = df_test['clean_text'].values\n",
    "y_test = df_test['Sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien nhan thanh dang onehotcoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = preprocessing.OneHotEncoder()\n",
    "y_train = ohe.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
    "y_valid = ohe.fit_transform(np.array(y_valid).reshape(-1, 1)).toarray()\n",
    "y_test = ohe.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chuan bi xay dung mo hinh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=128\n",
    "def tokenize(data,max_len=MAX_LEN) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for i in range(len(data)):\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            data[i],\n",
    "            add_special_tokens=True,\n",
    "            length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids),np.array(attention_masks)\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def create_model(bert_model, max_len=MAX_LEN):\n",
    "    \n",
    "    ##params###\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-7)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "\n",
    "    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
    "    \n",
    "    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
    "    \n",
    "    embeddings = bert_model([input_ids,attention_masks])[1]\n",
    "    \n",
    "    output = tf.keras.layers.Dense(3, activation=\"softmax\")(embeddings)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks], outputs = output)\n",
    "    \n",
    "    model.compile(opt, loss=loss, metrics=accuracy)\n",
    "\n",
    "[train_input_ids,train_attention_masks]=tokenize(X_train)\n",
    "[val_input_ids,val_attention_masks]=tokenize(X_valid)\n",
    "[test_input_ids,test_attention_masks]=tokenize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thuc hien viec training mo hinh voi du lieu da co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(bert_model, MAX_LEN)\n",
    "model.summary()\n",
    "history_bert = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_valid), epochs=4, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kiem tra mo hinh voi du lieu test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_bert = model.predict([test_input_ids,test_attention_masks])\n",
    "y_pred_bert=np.argmax(result_bert,axis=1)\n",
    "y_true_test=np.argmax(y_test,axis=1)\n",
    "print('\\tClassification Report for BERT:\\n\\n',classification_report(y_true_test,y_pred_bert, target_names=['Negative', 'Neutral', 'Positive']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.layers import Dense, ReLU, Flatten, LSTM, Input, MaxPooling1D, Lambda, MaxPooling2D\n",
    "from tensorflow.keras.layers import Input, Embedding, BatchNormalization, Concatenate, Reshape\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lấy data từ 2 file X_train Y_train X_test và Y_test từ các file X_train_cleaned Y_train_cleaned X_test_cleaned Y_test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr=pd.read_csv('Twitter_2/X_train_cleaned.csv',header=None,encoding='latin1')\n",
    "Y_tr=pd.read_csv('Twitter_2/Y_train_cleaned.csv',header=None)\n",
    "X_t=pd.read_csv('Twitter_2/X_test_cleaned.csv',header=None,encoding='latin1')\n",
    "Y_t=pd.read_csv('Twitter_2/Y_test_cleaned.csv',header=None)\n",
    "X_train = X_tr[0].astype(str).tolist()\n",
    "X_test = X_t[0].astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chuẩn bị data để nhét vào mô hình chuẩn bị việc train mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN.LAPTOP-6P42KB5P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\ADMIN.LAPTOP-6P42KB5P\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    }
   ],
   "source": [
    "encoder=LabelEncoder()\n",
    "Y_train=encoder.fit_transform(Y_tr)\n",
    "Y_test=encoder.transform(Y_t)\n",
    "tokenizer=Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences=tokenizer.texts_to_sequences(X_train)\n",
    "tr_x=pad_sequences(sequences,maxlen=50)\n",
    "tr_y=to_categorical(Y_train)\n",
    "sequences=tokenizer.texts_to_sequences(X_test)\n",
    "t_x=pad_sequences(sequences,maxlen=50)\n",
    "t_y=to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tạo mô hình CNN để chạy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "max_len=50\n",
    "max_word=20000\n",
    "batchsize=64\n",
    "epochs=25\n",
    "def model1():\n",
    "    inp=Input(shape=(max_len,))\n",
    "\n",
    "    embed1=Embedding(input_dim=max_word,output_dim=embedding_dim,input_length=max_len)(inp)\n",
    "    reshape=Reshape((max_len,embedding_dim,1))(embed1)\n",
    "    conv0=Conv2D(filters=100,kernel_size=(3,embedding_dim),activation='relu',kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "    conv1=Conv2D(filters=100,kernel_size=(4,embedding_dim),activation='relu',kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "    conv2=Conv2D(filters=100,kernel_size=(5,embedding_dim),activation='relu',kernel_regularizer=regularizers.l2(3))(reshape)\n",
    "    pool0=MaxPooling2D(pool_size=(max_len-3+1,1),strides=(1,1),padding='valid')(conv0)\n",
    "    pool1=MaxPooling2D(pool_size=(max_len-4+1,1),strides=(1,1),padding='valid')(conv1)\n",
    "    pool2=MaxPooling2D(pool_size=(max_len-5+1,1),strides=(1,1),padding='valid')(conv2)\n",
    "    merge=Concatenate(axis=1)([pool0,pool1,pool2])\n",
    "    flat=Flatten()(merge)\n",
    "    drop=Dropout(0.5)(flat)\n",
    "    outp=Dense(3,activation='softmax')(drop)\n",
    "    model=Model(inputs=inp,outputs=outp)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',Precision,Recall])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chạy mô hình với data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m      3\u001b[0m history\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(tr_x, tr_y, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatchsize, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m, in \u001b[0;36mmodel1\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel1\u001b[39m():\n\u001b[1;32m----> 7\u001b[0m     inp\u001b[38;5;241m=\u001b[39m\u001b[43mInput\u001b[49m(shape\u001b[38;5;241m=\u001b[39m(max_len,))\n\u001b[0;32m      9\u001b[0m     embed1\u001b[38;5;241m=\u001b[39mEmbedding(input_dim\u001b[38;5;241m=\u001b[39mmax_word,output_dim\u001b[38;5;241m=\u001b[39membedding_dim,input_length\u001b[38;5;241m=\u001b[39mmax_len)(inp)\n\u001b[0;32m     10\u001b[0m     reshape\u001b[38;5;241m=\u001b[39mReshape((max_len,embedding_dim,\u001b[38;5;241m1\u001b[39m))(embed1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "model=model1()\n",
    "model.summary()\n",
    "history=model.fit(tr_x, tr_y, epochs=epochs, batch_size=batchsize, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thử dữ liệu với data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label=model.predict(t_x)\n",
    "predicted=np.argmax(pred_label,axis=1)\n",
    "true=np.argmax(t_y,axis=1)\n",
    "print(classification_report(true, predicted))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
